# N-armed Bandits
This code replicates the 10-armed testbed used in the book to compare performance of ε-greedy algorithms with varying exploration rate ε.
